# -*- coding: utf-8 -*-
"""Goodreads.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eyn-AgBkPaCzjZDQKiQrq4ksCSTel3Aa
"""

#GoodReads Book Review
#Jack Moncur

#Imports
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import csv
from wordcloud import WordCloud
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, classification_report
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
import nltk
import re
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('stopwords')
nltk.download('vader_lexicon')

#Pre-Processing - Train data

# Set the input and output file paths
input_file = '/content/goodreads_train.csv'
output_file = 'csvnew.csv'

# Open the input and output files
with open(input_file, 'r', encoding='utf-8-sig') as csv_input_file, \
        open(output_file, 'w', encoding='utf-8-sig', newline='') as csv_output_file:

    # Create a CSV reader and writer objects
    csv_reader = csv.reader(csv_input_file)
    csv_writer = csv.writer(csv_output_file)

    # Keep track of any skipped rows
    skipped_rows = []

    # Iterate over the rows in the input file
    for row_num, row in enumerate(csv_reader, start=1):
        try:
            # Process the row if there are no errors
            # Replace this code with your own processing logic
            # Write the valid row to the output file
            csv_writer.writerow(row)
        
        except (UnicodeDecodeError, csv.Error) as e:
            # Skip the problematic row and keep track of its row number
            skipped_rows.append(row_num)
            print(f'Skipping row {row_num}: {e}')

    # Print a summary of the skipped rows, if any
    if skipped_rows:
        print(f'Skipped {len(skipped_rows)} rows: {skipped_rows}')
    else:
        print('All rows processed successfully.')

#Preprocessing - Test Data

# Set the input and output file paths
input_file = '/content/goodreads_test.csv'
output_file = 'csvnew_test.csv'


# Open the input and output files
with open(input_file, 'r', encoding='utf-8-sig') as csv_input_file, \
        open(output_file, 'w', encoding='utf-8-sig', newline='') as csv_output_file:

    # Create a CSV reader and writer objects
    csv_reader = csv.reader(csv_input_file)
    csv_writer = csv.writer(csv_output_file)

    # Keep track of any skipped rows
    skipped_rows = []

    # Iterate over the rows in the input file
    for row_num, row in enumerate(csv_reader, start=1):
        try:
            # Process the row if there are no errors
            # Replace this code with your own processing logic
            # Write the valid row to the output file
            csv_writer.writerow(row)
        
        except (UnicodeDecodeError, csv.Error) as e:
            # Skip the problematic row and keep track of its row number
            skipped_rows.append(row_num)
            print(f'Skipping row {row_num}: {e}')

    # Print a summary of the skipped rows, if any
    if skipped_rows:
        print(f'Skipped {len(skipped_rows)} rows: {skipped_rows}')
    else:
        print('All rows processed successfully.')

#Import Data
df_train = pd.read_csv('/content/csvnew.csv', error_bad_lines=False)

df_train.head()

#Original Review

print(df_train['review_text'][0])

#Import Test Data
df_test = pd.read_csv('/content/goodreads_test.csv', error_bad_lines=False)

train_columns = set(df_train.columns)
test_columns = set(df_test.columns)

# Check which columns are missing in the train dataframe
missing_columns_train = test_columns.difference(train_columns)
print("Columns missing in train dataframe:", missing_columns_train)

# Check which columns are missing in the test dataframe
missing_columns_test = train_columns.difference(test_columns)
print("Columns missing in test dataframe:", missing_columns_test)

#Test Head
df_test.head()

#Drop Un-needed info

df_train.drop(['user_id','review_id','date_added','date_updated','read_at','started_at'],axis=1,inplace=True)
df_train.head()

#Clean up the Reviews

#Remove Stopwords
stop_words = list(stopwords.words('english'))
df_train['review_text'] = df_train['review_text'].apply(lambda x: ' '.join([i for i in x.split() if i not in stop_words]))

#Remove Repeated Words
df_train['review_text'] = df_train['review_text'].apply(lambda x: ' '.join(set(x.split())))

#Remove Small words < 5 characters 
df_train['review_text'] = df_train['review_text'].apply(lambda x: ' '.join([i for i in x.split() if len(i) >= 5]))

#Remove Hyperlinks
df_train['review_text'] = df_train['review_text'].apply(
    lambda x: re.sub(
        r'(https?:\/\/)(\s)*(www\.)?(\s)*((\w|\s)+\.)*([\w\-\s]+\/)*([\w\-]+)((\?)?[\w\s]*=\s*[\w\%&]*)*', '',
        x, flags=re.MULTILINE))

#Remove Special Characters
df_train['review_text'] = df_train['review_text'].apply(lambda x: re.sub('\W+',' ', x))

#Remove extra spaces / line breaks
df_train['review_text'] = df_train['review_text'].apply(lambda x: " ".join(x.split()))

#All lowercase
df_train['review_text'] = df_train['review_text'].apply(lambda x: str(x).lower())

#Head

df_train.head()

#Check Review

print(df_train['review_text'][0])

#Word Cloud

text = " ".join(review for review in df_train.review_text)

word_cloud = WordCloud(width = 800, height=400, max_font_size=50, max_words=50, background_color="white").generate(text)

plt.figure(figsize=(12, 10))
plt.imshow(word_cloud, interpolation="bilinear")
plt.axis("off")
plt.show()

#Analyze Sentiment of Reviews

df_train['review_text'] = df_train['review_text'].astype(str)

SA = SentimentIntensityAnalyzer()

positive_scores = []
negative_scores = []
neutral_scores = []

for review in df_train['review_text']:
    sentiment_scores = SA.polarity_scores(review)
    positive_scores.append(sentiment_scores['pos'])
    negative_scores.append(sentiment_scores['neg'])
    neutral_scores.append(sentiment_scores['neu'])

df_train['Positive'] = positive_scores
df_train['Negative'] = negative_scores
df_train['Neutral'] = neutral_scores

#Polarity Scores head
df_train.head()

#Visual showing overall sentiment

sns.set(style="whitegrid")

avg_positive = df_train["Positive"].mean()
avg_negative = df_train["Negative"].mean()
avg_neutral = df_train["Neutral"].mean()

vis = sns.barplot(x=["positive", "negative", "neutral"], y=[avg_positive, avg_negative, avg_neutral])
vis.set_title("Total Average Review Sentiment Score")
vis.set_ylabel("Average Sentiement Score")
vis.set_xlabel("Sentiment")
plt.show()

#Drop Nulls
df_train.dropna(inplace=True)

#Check Nulls
df_train.isnull().sum()

# Linear Regression

df_train[['n_votes', 'n_comments', 'Positive', 'Negative', 'Neutral']] = df_train[['n_votes', 'n_comments', 'Positive', 'Negative', 'Neutral']].astype(int)

X = df_train[['n_votes', 'n_comments', 'Positive', 'Negative', 'Neutral']]
y = df_train['rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
print('R2 Score:', r2)

#Random Forest

X = df_train[['n_votes', 'n_comments', 'Positive', 'Negative', 'Neutral']]
y = df_train['rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print('R2 Score:', r2)
print('MSE:', mse)
print('RMSE:', rmse)
print(classification_report(y_test, np.round(y_pred)))

#Another Attempt - Get a better accuracy score!

labels = np.array(df_train['rating'])

features = df_train.drop(['n_votes', 'n_comments', 'book_id', 'rating', 'review_text'], axis = 1)
features = np.array(features)

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.75, random_state=42)

d_tree = tree.DecisionTreeClassifier(max_depth = 3, random_state = 42).fit(X_train, y_train)

print(classification_report(y_test, np.round(d_tree.predict(X_test))))

#Random Forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

labels = np.array(df_train['rating'])
features = df_train.drop(['n_votes', 'n_comments', 'book_id', 'rating', 'review_text'], axis=1)
features = np.array(features)

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.9, random_state=1)

rfc = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)
rfc.fit(X_train, y_train)

print(classification_report(y_test, np.round(rfc.predict(X_test))))

#Support Vector

from sklearn.svm import SVC
from sklearn.metrics import classification_report

labels = np.array(df_train['rating'])
features = df_train.drop(['n_votes', 'n_comments', 'book_id','rating', 'review_text'], axis=1)
features = np.array(features)

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.75, random_state=42)

svc = SVC(kernel='linear', C=1.0, random_state=42)
svc.fit(X_train, y_train)

print(classification_report(y_test, np.round(svc.predict(X_test))))

#Neural Network

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report

labels = np.array(df_train['rating'])
features = df_train.drop(['n_votes', 'n_comments', 'book_id', 'rating', 'review_text'], axis=1)
features = np.array(features)

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.75, random_state=42)

model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=5)

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, callbacks=[early_stopping])

y_pred = np.round(model.predict(X_test))

print(classification_report(y_test, y_pred))